---
title: "Data Cleaning & Pre-Processing"
subtitle: "Preparing and Standardizing Lightcast Job Postings Data for Analysis"
bibliography: references.bib
csl: csl/econometrica.csl
format:
  html:
    toc: true
    code-fold: true
    code-tools: true
---

<div class="card reveal">

## Objective

Data cleaning is the most critical phase in any analytics workflow.
Following the AD688 project requirements, our goals for this section are to:

- inspect the raw dataset structure,
- identify and remove redundant or outdated columns,
- fix inconsistent formatting (e.g., stringified list fields),
- standardize datetime fields and create analysis-ready variables,
- handle missing values using appropriate statistical strategies,
- remove duplicate job postings,
- export a clean, consistent dataset used across EDA, ML, and NLP sections.

This ensures accuracy, reproducibility, and coherence across the entire site.

</div>

---

<div class="card reveal">

### Initial Dataset Overview

```{python}
import pandas as pd

df = pd.read_csv("data/lightcast_job_postings.csv", low_memory=False)

df.head()
```

```{python}
# Dataset structure
df.info()
```

```{python}
# List all column names
list(df.columns)
```

</div>

---

<div class="card reveal">

### Dropping Redundant or Outdated Columns

Lightcast contains multiple versions of NAICS/SOC classifications, plus metadata fields (tracking URLs, flags, timestamps).
Thus, we remove irrelevant or outdated fields.

```{python}
columns_to_drop = [
    "ID", "URL", "ACTIVE_URLS", "DUPLICATES", "LAST_UPDATED_TIMESTAMP",
    "NAICS2","NAICS3","NAICS4","NAICS5","NAICS6",
    "SOC_2","SOC_3","SOC_5"
]

df.drop(columns=columns_to_drop, inplace=True, errors="ignore")
df.head()
```

</div>

---

<div class="card reveal">

### Cleaning Messy List-Type Columns

Many Lightcast columns store lists as text such as:

["Job Board"]
["disabledperson.com", "dejobs.org"]

So, we convert these into clean comma-separated strings.

```{python}
import ast, re

def clean_list(x):
    if pd.isna(x):
        return ""
    x = x.replace("\n"," ").strip()
    try:
        obj = ast.literal_eval(x)
        if isinstance(obj, list):
            return ", ".join(str(i).strip() for i in obj)
    except:
        pass
    return re.sub(r'[$begin:math:display$$end:math:display$\"]',"",x).strip()

list_cols = [
    "SOURCE_TYPES","SOURCES","SKILLS_NAME","SPECIALIZED_SKILLS_NAME",
    "COMMON_SKILLS_NAME","SOFTWARE_SKILLS_NAME","CERTIFICATIONS_NAME"
]

for col in list_cols:
    if col in df.columns:
        df[col] = df[col].apply(clean_list)
```

</div>

---

<div class="card reveal">

### Handling Missing Values

<div class="card reveal">

Understanding missingness is important for determining how to impute values and whether key fields are usable.

We focus only on essential analytical fields:

- Salary fields  
- Experience requirements  
- Location fields  
- Duration  

We compute missingness correlations **only for columns that still exist after cleaning.**

```{python}
# Identify most-missing columns (overview)
missing_summary = df.isna().mean().sort_values(ascending=False).head(15)
missing_summary
```

```{python}
# Drop columns with more than 50% missing values
df.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)

# Fill numeric values with median
num_cols = df.select_dtypes(include=["float","int"]).columns
df[num_cols] = df[num_cols].fillna(df[num_cols].median())

# Fill categorical values with "Unknown"
cat_cols = df.select_dtypes(include="object").columns
df[cat_cols] = df[cat_cols].fillna("Unknown")
```

```{python}
print("Shape:", df.shape)
display(df.head())
df.info()
```

</div>

---

<div class="card reveal">

### Standardizing the POSTED Date and Creating a Month Column

Datetime consistency is essential for trend analysis and for joining with FRED macroeconomic indicators.

```{python}
df["POSTED"] = pd.to_datetime(df["POSTED"], errors="coerce")
df["month"] = df["POSTED"].dt.to_period("M").astype(str)

df[["POSTED", "month"]].head()
```

</div>

---

<div class="card reveal">

### Removing Duplicate Job Postings

Duplicate postings can overcount job demand and distort salary statistics.
We remove duplicates based on:

- job title  
- company  
- location  
- original posting date  

```{python}
subset_cols = [c for c in ["TITLE_NAME", "COMPANY_NAME", "LOCATION", "POSTED"]
               if c in df.columns]

if subset_cols:
    df = df.drop_duplicates(subset=subset_cols, keep="first")

df.shape
```

</div>

---

<div class="card reveal">

### Exporting the Final Cleaned Dataset

This cleaned dataset will be used consistently across:

- EDA  
- Skill Gap Analysis  
- ML Methods  
- NLP Methods  

Exporting ensures team alignment and prevents inconsistent results.

```{python}
df.to_csv("data/lightcast_cleaned.csv", index=False)

df.head(), df.shape
```

</div>

---

<div class="card reveal">

## Summary

This cleaned dataset establishes the foundation for all downstream analysis.
By standardizing the cleaning workflow and exporting a unified dataset, we ensure:

- accuracy and reproducibility,  
- consistent modeling results,  
- reduced rendering load for GitHub Pages,  
- smooth integration with the ML and NLP pipelines.

This completes the **Data Cleaning & Exploration** phase.

</div>