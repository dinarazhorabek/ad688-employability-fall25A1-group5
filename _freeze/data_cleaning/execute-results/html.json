{
  "hash": "7fea98729aca7a0715759ef99b50f51f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Data Cleaning & Pre-Processing\"\nsubtitle: \"Preparing and Standardizing Lightcast Job Postings Data for Analysis\"\nbibliography: references.bib\ncsl: csl/econometrica.csl\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-tools: true\n---\n\n<div class=\"card reveal\">\n\n## Objective\n\nData cleaning is the most critical phase in any analytics workflow.  \nFollowing the AD688 project requirements, our goals for this section are to:\n\n- inspect the raw dataset structure,  \n- identify and remove redundant or outdated columns,  \n- fix inconsistent formatting (e.g., stringified list fields),  \n- **standardize salary fields and preserve them even when sparsely reported**,  \n- standardize datetime fields and create analysis-ready variables,  \n- handle missing values using appropriate statistical strategies,  \n- remove duplicate job postings,  \n- export a clean, consistent dataset used across EDA, ML, and NLP sections.\n\nThis ensures accuracy, reproducibility, and coherence across the entire site.\n\n</div>\n\n---\n\n<div class=\"card reveal\">\n\n### Initial Dataset Overview\n\n::: {#8a7f840c .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport ast\nimport re\n\n# Load raw Lightcast job postings\ndf = pd.read_csv(\"data/lightcast_job_postings.csv\", low_memory=False)\n\n# Basic structural overview\n# df.head()\n# df.info()\n# list(df.columns)\n```\n:::\n\n\n</div>\n\n---\n\n<div class=\"card reveal\">\n\n### Dropping Redundant or Outdated Columns\n\nLightcast contains multiple versions of NAICS/SOC classifications, plus metadata fields (tracking URLs, flags, timestamps).\nWe remove clearly redundant metadata and older classification versions while keeping the 2022 NAICS codes and 2021 SOC codes.\n\n::: {#6dbbb02e .cell execution_count=2}\n``` {.python .cell-code}\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\ndf.drop(columns=columns_to_drop, inplace=True, errors=\"ignore\")\n\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LAST_UPDATED_DATE</th>\n      <th>POSTED</th>\n      <th>EXPIRED</th>\n      <th>DURATION</th>\n      <th>SOURCE_TYPES</th>\n      <th>SOURCES</th>\n      <th>ACTIVE_SOURCES_INFO</th>\n      <th>TITLE_RAW</th>\n      <th>BODY</th>\n      <th>MODELED_EXPIRED</th>\n      <th>...</th>\n      <th>NAICS_2022_2</th>\n      <th>NAICS_2022_2_NAME</th>\n      <th>NAICS_2022_3</th>\n      <th>NAICS_2022_3_NAME</th>\n      <th>NAICS_2022_4</th>\n      <th>NAICS_2022_4_NAME</th>\n      <th>NAICS_2022_5</th>\n      <th>NAICS_2022_5_NAME</th>\n      <th>NAICS_2022_6</th>\n      <th>NAICS_2022_6_NAME</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9/6/2024</td>\n      <td>6/2/2024</td>\n      <td>6/8/2024</td>\n      <td>6.0</td>\n      <td>[\\n  \"Company\"\\n]</td>\n      <td>[\\n  \"brassring.com\"\\n]</td>\n      <td>NaN</td>\n      <td>Enterprise Analyst (II-III)</td>\n      <td>31-May-2024\\n\\nEnterprise Analyst (II-III)\\n\\n...</td>\n      <td>6/8/2024</td>\n      <td>...</td>\n      <td>44.0</td>\n      <td>Retail Trade</td>\n      <td>441.0</td>\n      <td>Motor Vehicle and Parts Dealers</td>\n      <td>4413.0</td>\n      <td>Automotive Parts, Accessories, and Tire Retailers</td>\n      <td>44133.0</td>\n      <td>Automotive Parts and Accessories Retailers</td>\n      <td>441330.0</td>\n      <td>Automotive Parts and Accessories Retailers</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8/2/2024</td>\n      <td>6/2/2024</td>\n      <td>8/1/2024</td>\n      <td>NaN</td>\n      <td>[\\n  \"Job Board\"\\n]</td>\n      <td>[\\n  \"maine.gov\"\\n]</td>\n      <td>NaN</td>\n      <td>Oracle Consultant - Reports (3592)</td>\n      <td>Oracle Consultant - Reports (3592)\\n\\nat SMX i...</td>\n      <td>8/1/2024</td>\n      <td>...</td>\n      <td>56.0</td>\n      <td>Administrative and Support and Waste Managemen...</td>\n      <td>561.0</td>\n      <td>Administrative and Support Services</td>\n      <td>5613.0</td>\n      <td>Employment Services</td>\n      <td>56132.0</td>\n      <td>Temporary Help Services</td>\n      <td>561320.0</td>\n      <td>Temporary Help Services</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9/6/2024</td>\n      <td>6/2/2024</td>\n      <td>7/7/2024</td>\n      <td>35.0</td>\n      <td>[\\n  \"Job Board\"\\n]</td>\n      <td>[\\n  \"dejobs.org\"\\n]</td>\n      <td>NaN</td>\n      <td>Data Analyst</td>\n      <td>Taking care of people is at the heart of every...</td>\n      <td>6/10/2024</td>\n      <td>...</td>\n      <td>52.0</td>\n      <td>Finance and Insurance</td>\n      <td>524.0</td>\n      <td>Insurance Carriers and Related Activities</td>\n      <td>5242.0</td>\n      <td>Agencies, Brokerages, and Other Insurance Rela...</td>\n      <td>52429.0</td>\n      <td>Other Insurance Related Activities</td>\n      <td>524291.0</td>\n      <td>Claims Adjusting</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9/6/2024</td>\n      <td>6/2/2024</td>\n      <td>7/20/2024</td>\n      <td>48.0</td>\n      <td>[\\n  \"Job Board\"\\n]</td>\n      <td>[\\n  \"disabledperson.com\",\\n  \"dejobs.org\"\\n]</td>\n      <td>NaN</td>\n      <td>Sr. Lead Data Mgmt. Analyst - SAS Product Owner</td>\n      <td>About this role:\\n\\nWells Fargo is looking for...</td>\n      <td>6/12/2024</td>\n      <td>...</td>\n      <td>52.0</td>\n      <td>Finance and Insurance</td>\n      <td>522.0</td>\n      <td>Credit Intermediation and Related Activities</td>\n      <td>5221.0</td>\n      <td>Depository Credit Intermediation</td>\n      <td>52211.0</td>\n      <td>Commercial Banking</td>\n      <td>522110.0</td>\n      <td>Commercial Banking</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6/19/2024</td>\n      <td>6/2/2024</td>\n      <td>6/17/2024</td>\n      <td>15.0</td>\n      <td>[\\n  \"FreeJobBoard\"\\n]</td>\n      <td>[\\n  \"craigslist.org\"\\n]</td>\n      <td>NaN</td>\n      <td>Comisiones de $1000 - $3000 por semana... Comi...</td>\n      <td>Comisiones de $1000 - $3000 por semana... Comi...</td>\n      <td>6/17/2024</td>\n      <td>...</td>\n      <td>99.0</td>\n      <td>Unclassified Industry</td>\n      <td>999.0</td>\n      <td>Unclassified Industry</td>\n      <td>9999.0</td>\n      <td>Unclassified Industry</td>\n      <td>99999.0</td>\n      <td>Unclassified Industry</td>\n      <td>999999.0</td>\n      <td>Unclassified Industry</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 118 columns</p>\n</div>\n```\n:::\n:::\n\n\n</div>\n\n---\n\n<div class=\"card reveal\">\n\n### Cleaning Messy List-Type Columns\n\nMany Lightcast columns store lists as text such as:\n\n[\"Job Board\"]\n[\"disabledperson.com\", \"dejobs.org\"]\n\nSo, we convert these into clean comma-separated strings.\n\n::: {#4c8ecf21 .cell execution_count=3}\n``` {.python .cell-code}\ndef clean_list(x):\n    \"\"\"\n    Convert stringified Python lists into comma-separated strings and strip noise.\n    If parsing fails, fall back to a lightly cleaned string.\n    \"\"\"\n    if pd.isna(x):\n        return \"\"\n    x = str(x).replace(\"\\n\", \" \").strip()\n    try:\n        obj = ast.literal_eval(x)\n        if isinstance(obj, list):\n            return \", \".join(str(i).strip() for i in obj)\n    except Exception:\n        pass\n    # Fallback: remove stray quotes if present\n    x = re.sub(r'[\"\\']', \"\", x)\n    return x.strip()\n\nlist_cols = [\n    \"SOURCE_TYPES\", \"SOURCES\",\n    \"SKILLS_NAME\", \"SPECIALIZED_SKILLS_NAME\",\n    \"COMMON_SKILLS_NAME\", \"SOFTWARE_SKILLS_NAME\",\n    \"CERTIFICATIONS_NAME\"\n]\n\nfor col in list_cols:\n    if col in df.columns:\n        df[col] = df[col].apply(clean_list)\n\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LAST_UPDATED_DATE</th>\n      <th>POSTED</th>\n      <th>EXPIRED</th>\n      <th>DURATION</th>\n      <th>SOURCE_TYPES</th>\n      <th>SOURCES</th>\n      <th>ACTIVE_SOURCES_INFO</th>\n      <th>TITLE_RAW</th>\n      <th>BODY</th>\n      <th>MODELED_EXPIRED</th>\n      <th>...</th>\n      <th>NAICS_2022_2</th>\n      <th>NAICS_2022_2_NAME</th>\n      <th>NAICS_2022_3</th>\n      <th>NAICS_2022_3_NAME</th>\n      <th>NAICS_2022_4</th>\n      <th>NAICS_2022_4_NAME</th>\n      <th>NAICS_2022_5</th>\n      <th>NAICS_2022_5_NAME</th>\n      <th>NAICS_2022_6</th>\n      <th>NAICS_2022_6_NAME</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9/6/2024</td>\n      <td>6/2/2024</td>\n      <td>6/8/2024</td>\n      <td>6.0</td>\n      <td>Company</td>\n      <td>brassring.com</td>\n      <td>NaN</td>\n      <td>Enterprise Analyst (II-III)</td>\n      <td>31-May-2024\\n\\nEnterprise Analyst (II-III)\\n\\n...</td>\n      <td>6/8/2024</td>\n      <td>...</td>\n      <td>44.0</td>\n      <td>Retail Trade</td>\n      <td>441.0</td>\n      <td>Motor Vehicle and Parts Dealers</td>\n      <td>4413.0</td>\n      <td>Automotive Parts, Accessories, and Tire Retailers</td>\n      <td>44133.0</td>\n      <td>Automotive Parts and Accessories Retailers</td>\n      <td>441330.0</td>\n      <td>Automotive Parts and Accessories Retailers</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8/2/2024</td>\n      <td>6/2/2024</td>\n      <td>8/1/2024</td>\n      <td>NaN</td>\n      <td>Job Board</td>\n      <td>maine.gov</td>\n      <td>NaN</td>\n      <td>Oracle Consultant - Reports (3592)</td>\n      <td>Oracle Consultant - Reports (3592)\\n\\nat SMX i...</td>\n      <td>8/1/2024</td>\n      <td>...</td>\n      <td>56.0</td>\n      <td>Administrative and Support and Waste Managemen...</td>\n      <td>561.0</td>\n      <td>Administrative and Support Services</td>\n      <td>5613.0</td>\n      <td>Employment Services</td>\n      <td>56132.0</td>\n      <td>Temporary Help Services</td>\n      <td>561320.0</td>\n      <td>Temporary Help Services</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9/6/2024</td>\n      <td>6/2/2024</td>\n      <td>7/7/2024</td>\n      <td>35.0</td>\n      <td>Job Board</td>\n      <td>dejobs.org</td>\n      <td>NaN</td>\n      <td>Data Analyst</td>\n      <td>Taking care of people is at the heart of every...</td>\n      <td>6/10/2024</td>\n      <td>...</td>\n      <td>52.0</td>\n      <td>Finance and Insurance</td>\n      <td>524.0</td>\n      <td>Insurance Carriers and Related Activities</td>\n      <td>5242.0</td>\n      <td>Agencies, Brokerages, and Other Insurance Rela...</td>\n      <td>52429.0</td>\n      <td>Other Insurance Related Activities</td>\n      <td>524291.0</td>\n      <td>Claims Adjusting</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9/6/2024</td>\n      <td>6/2/2024</td>\n      <td>7/20/2024</td>\n      <td>48.0</td>\n      <td>Job Board</td>\n      <td>disabledperson.com, dejobs.org</td>\n      <td>NaN</td>\n      <td>Sr. Lead Data Mgmt. Analyst - SAS Product Owner</td>\n      <td>About this role:\\n\\nWells Fargo is looking for...</td>\n      <td>6/12/2024</td>\n      <td>...</td>\n      <td>52.0</td>\n      <td>Finance and Insurance</td>\n      <td>522.0</td>\n      <td>Credit Intermediation and Related Activities</td>\n      <td>5221.0</td>\n      <td>Depository Credit Intermediation</td>\n      <td>52211.0</td>\n      <td>Commercial Banking</td>\n      <td>522110.0</td>\n      <td>Commercial Banking</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6/19/2024</td>\n      <td>6/2/2024</td>\n      <td>6/17/2024</td>\n      <td>15.0</td>\n      <td>FreeJobBoard</td>\n      <td>craigslist.org</td>\n      <td>NaN</td>\n      <td>Comisiones de $1000 - $3000 por semana... Comi...</td>\n      <td>Comisiones de $1000 - $3000 por semana... Comi...</td>\n      <td>6/17/2024</td>\n      <td>...</td>\n      <td>99.0</td>\n      <td>Unclassified Industry</td>\n      <td>999.0</td>\n      <td>Unclassified Industry</td>\n      <td>9999.0</td>\n      <td>Unclassified Industry</td>\n      <td>99999.0</td>\n      <td>Unclassified Industry</td>\n      <td>999999.0</td>\n      <td>Unclassified Industry</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 118 columns</p>\n</div>\n```\n:::\n:::\n\n\n</div>\n\n---\n\n<div class=\"card reveal\">\n\nSalary Fields: Cleaning and Preservation\n\nSalary information is central to our job-market analysis, so we explicitly clean and preserve all salary-related fields, even when they are sparsely populated.\n\nWe:\n\n\t•\tidentify key salary-like columns: SALARY_FROM, SALARY_TO, SALARY, ORIGINAL_PAY_PERIOD,\n\t•\tstrip currency symbols, commas, and non-numeric characters from numeric fields,\n\t•\tcreate a standardized PAY_PERIOD,\n\t•\tconvert everything to an annualized ANNUAL_SALARY field suitable for EDA and modeling.\n\n::: {#ef6346c4 .cell execution_count=4}\n``` {.python .cell-code}\n# Key salary-related columns we want to preserve\nsalary_cols = [\"SALARY_FROM\", \"SALARY_TO\", \"SALARY\", \"ORIGINAL_PAY_PERIOD\"]\n\n# Ensure the columns exist\nfor col in salary_cols:\n    if col not in df.columns:\n        df[col] = np.nan\n\n# Helper to clean salary values, including ranges like \"90k-120k\"\ndef clean_salary_range(x):\n    if pd.isna(x):\n        return np.nan\n    x = str(x)\n\n    # Keep digits, commas, dot, minus, and en-dash\n    x = re.sub(r\"[^\\d,\\.\\-\\–]\", \"\", x)\n\n    # If there is a range, split and average\n    if \"-\" in x or \"–\" in x:\n        parts = re.split(r\"[-–]\", x)\n        nums = []\n        for p in parts:\n            p = p.replace(\",\", \"\")\n            if p == \"\":\n                continue\n            try:\n                nums.append(float(p))\n            except Exception:\n                pass\n        if len(nums) >= 2:\n            return np.mean(nums)\n        elif len(nums) == 1:\n            return nums[0]\n        return np.nan\n\n    # Otherwise, treat as a single value\n    x = x.replace(\",\", \"\")\n    if x == \"\":\n        return np.nan\n    try:\n        return float(x)\n    except Exception:\n        return np.nan\n\n# Clean numeric salary fields (FROM, TO, SALARY)\nnumeric_salary_fields = [\"SALARY_FROM\", \"SALARY_TO\", \"SALARY\"]\n\nfor col in numeric_salary_fields:\n    df[col] = df[col].apply(clean_salary_range)\n\n# Choose the best base salary for each row\ndef get_base_salary(row):\n    # 1. If SALARY exists, treat it as already standardized\n    if pd.notna(row[\"SALARY\"]):\n        return row[\"SALARY\"]\n    # 2. If both FROM and TO exist, average them\n    if pd.notna(row[\"SALARY_FROM\"]) and pd.notna(row[\"SALARY_TO\"]):\n        return (row[\"SALARY_FROM\"] + row[\"SALARY_TO\"]) / 2\n    # 3. Fallback to whichever exists\n    if pd.notna(row[\"SALARY_FROM\"]):\n        return row[\"SALARY_FROM\"]\n    if pd.notna(row[\"SALARY_TO\"]):\n        return row[\"SALARY_TO\"]\n    return np.nan\n\n# Normalize pay period strings\ndef normalize_period(x):\n    if pd.isna(x):\n        return \"year\"\n    x = str(x).lower()\n\n    if any(k in x for k in [\"year\", \"yr\", \"annual\"]):\n        return \"year\"\n    if any(k in x for k in [\"month\", \"mo\", \"per_month\"]):\n        return \"month\"\n    if \"biweek\" in x:\n        return \"biweek\"\n    if any(k in x for k in [\"week\", \"wk\", \"per_week\"]):\n        return \"week\"\n    if any(k in x for k in [\"day\", \"daily\"]):\n        return \"day\"\n    if any(k in x for k in [\"hour\", \"hr\"]):\n        return \"hour\"\n    return \"year\"\n\ndf[\"PAY_PERIOD\"] = df[\"ORIGINAL_PAY_PERIOD\"].apply(normalize_period)\n\n# Convert base salary to annual salary\ndef convert_to_annual(row):\n    salary = get_base_salary(row)\n    period = row[\"PAY_PERIOD\"]\n\n    if pd.isna(salary):\n        return np.nan\n\n    if period == \"year\":\n        return salary\n    if period == \"month\":\n        return salary * 12\n    if period == \"week\":\n        return salary * 52\n    if period == \"biweek\":\n        return salary * 26\n    if period == \"day\":\n        return salary * 260   # approx. work days in a year\n    if period == \"hour\":\n        return salary * 2080  # 40 hours * 52 weeks\n    return salary\n\ndf[\"ANNUAL_SALARY\"] = df.apply(convert_to_annual, axis=1)\n\n# Remove implausible values (e.g., < $10k or > $700k per year)\ndf.loc[df[\"ANNUAL_SALARY\"] < 10000, \"ANNUAL_SALARY\"] = np.nan\ndf.loc[df[\"ANNUAL_SALARY\"] > 700000, \"ANNUAL_SALARY\"] = np.nan\n\ndf[\"ANNUAL_SALARY\"] = df[\"ANNUAL_SALARY\"].astype(float)\n\ndf[[\"SALARY_FROM\", \"SALARY_TO\", \"SALARY\", \"PAY_PERIOD\", \"ANNUAL_SALARY\"]].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SALARY_FROM</th>\n      <th>SALARY_TO</th>\n      <th>SALARY</th>\n      <th>PAY_PERIOD</th>\n      <th>ANNUAL_SALARY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>year</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>year</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>year</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>year</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>35000.0</td>\n      <td>150000.0</td>\n      <td>92500.0</td>\n      <td>year</td>\n      <td>92500.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n</div>\n\n<div class=\"card reveal\">\n\n### Handling Missing Values\n\nUnderstanding missingness is important for determining how to impute values and whether key fields are usable.\n\nWe focus on essential analytical fields:\n\n\t•\tsalary fields (now cleaned),\n\t•\texperience requirements,\n\t•\tlocation fields,\n\t•\tduration.\n\nWe drop extremely sparse columns (more than 50% missing) but never drop salary-related fields, even if they exceed this threshold.\n\n::: {#346fcb88 .cell execution_count=5}\n``` {.python .cell-code}\n# Inspect the most-missing columns\nmissing_summary = df.isna().mean().sort_values(ascending=False).head(15)\nmissing_summary\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nACTIVE_SOURCES_INFO       0.892163\nMAX_YEARS_EXPERIENCE      0.883721\nMAX_EDULEVELS_NAME        0.774959\nMAX_EDULEVELS             0.774959\nLIGHTCAST_SECTORS         0.754655\nLIGHTCAST_SECTORS_NAME    0.754655\nANNUAL_SALARY             0.672377\nSALARY                    0.575050\nSALARY_FROM               0.553119\nSALARY_TO                 0.553119\nORIGINAL_PAY_PERIOD       0.553119\nDURATION                  0.376783\nMIN_YEARS_EXPERIENCE      0.319264\nMODELED_DURATION          0.266076\nMODELED_EXPIRED           0.212640\ndtype: float64\n```\n:::\n:::\n\n\n::: {#bd7261c7 .cell execution_count=6}\n``` {.python .cell-code}\n# Critical columns that must be preserved\ncritical_cols = [\n    c for c in ([\"SALARY_FROM\", \"SALARY_TO\", \"SALARY\", \"ORIGINAL_PAY_PERIOD\",\n                 \"ANNUAL_SALARY\", \"PAY_PERIOD\"])\n    if c in df.columns\n]\n\nthreshold = len(df) * 0.5\n\n# Drop columns with >50% missing, except critical salary / pay-period fields\ncols_to_drop_missing = [\n    c for c in df.columns\n    if c not in critical_cols and df[c].isna().sum() > threshold\n]\n\ndf.drop(columns=cols_to_drop_missing, inplace=True)\n\n# Fill numeric values with median (EXCLUDING salary-related fields)\nnum_cols = df.select_dtypes(include=[\"float\", \"int\"]).columns\nnum_exclude = set([\"SALARY_FROM\", \"SALARY_TO\", \"SALARY\", \"ANNUAL_SALARY\"])\nnum_fill = [c for c in num_cols if c not in num_exclude]\n\ndf[num_fill] = df[num_fill].fillna(df[num_fill].median())\n\n# Leave salary-related fields as NaN when missing\n\n# Fill categorical values with \"Unknown\"\ncat_cols = df.select_dtypes(include=\"object\").columns\ndf[cat_cols] = df[cat_cols].fillna(\"Unknown\")\n\nprint(\"Shape after missing-value handling:\", df.shape)\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape after missing-value handling: (72498, 114)\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 72498 entries, 0 to 72497\nColumns: 114 entries, LAST_UPDATED_DATE to ANNUAL_SALARY\ndtypes: float64(31), object(83)\nmemory usage: 63.1+ MB\n```\n:::\n:::\n\n\n</div>\n\n---\n\n<div class=\"card reveal\">\n\n### Standardizing the POSTED Date and Creating a Month Column\n\nDatetime consistency is essential for trend analysis and for joining with FRED macroeconomic indicators.\n\n::: {#4c5cb924 .cell execution_count=7}\n``` {.python .cell-code}\ndf[\"POSTED\"] = pd.to_datetime(df[\"POSTED\"], errors=\"coerce\")\ndf[\"month\"] = df[\"POSTED\"].dt.to_period(\"M\").astype(str)\n\ndf[[\"POSTED\", \"month\"]].head()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/f7/1k5swm_52594v7sfs78dy91h0000gn/T/ipykernel_42669/3881994798.py:2: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>POSTED</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2024-06-02</td>\n      <td>2024-06</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2024-06-02</td>\n      <td>2024-06</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2024-06-02</td>\n      <td>2024-06</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2024-06-02</td>\n      <td>2024-06</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2024-06-02</td>\n      <td>2024-06</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n</div>\n\n---\n\n<div class=\"card reveal\">\n\n### Removing Duplicate Job Postings\n\nDuplicate postings can overcount job demand and distort salary statistics.\nWe remove duplicates based on:\n\n- job title  \n- company  \n- location  \n- original posting date  \n\n::: {#e15e92ad .cell execution_count=8}\n``` {.python .cell-code}\nsubset_cols = [c for c in [\"TITLE_NAME\", \"COMPANY_NAME\", \"LOCATION\", \"POSTED\"]\n               if c in df.columns]\n\nif subset_cols:\n    before = df.shape[0]\n    df = df.drop_duplicates(subset=subset_cols, keep=\"first\")\n    after = df.shape[0]\n    print(f\"Removed {before - after} duplicate postings.\")\nelse:\n    print(\"Duplicate removal skipped: key columns missing.\")\n\ndf.shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemoved 3300 duplicate postings.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n(69198, 115)\n```\n:::\n:::\n\n\n</div>\n\n---\n\n<div class=\"card reveal\">\n\n### Exporting the Final Cleaned Dataset\n\nExporting the Final Cleaned Dataset\n\nThis cleaned dataset will be used consistently across:\n\n\t•\tExploratory Data Analysis (EDA),\n\t•\tSkill Gap Analysis,\n\t•\tML Methods,\n\t•\tNLP Methods.\n\nExporting ensures team alignment and prevents inconsistent results.\n\n::: {#c835270a .cell execution_count=9}\n``` {.python .cell-code}\ndf.to_csv(\"data/lightcast_cleaned.csv\", index=False)\n\ndf.head(), df.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n(  LAST_UPDATED_DATE     POSTED    EXPIRED  DURATION  SOURCE_TYPES  \\\n 0          9/6/2024 2024-06-02   6/8/2024       6.0       Company   \n 1          8/2/2024 2024-06-02   8/1/2024      18.0     Job Board   \n 2          9/6/2024 2024-06-02   7/7/2024      35.0     Job Board   \n 3          9/6/2024 2024-06-02  7/20/2024      48.0     Job Board   \n 4         6/19/2024 2024-06-02  6/17/2024      15.0  FreeJobBoard   \n \n                           SOURCES  \\\n 0                   brassring.com   \n 1                       maine.gov   \n 2                      dejobs.org   \n 3  disabledperson.com, dejobs.org   \n 4                  craigslist.org   \n \n                                            TITLE_RAW  \\\n 0                        Enterprise Analyst (II-III)   \n 1                 Oracle Consultant - Reports (3592)   \n 2                                       Data Analyst   \n 3    Sr. Lead Data Mgmt. Analyst - SAS Product Owner   \n 4  Comisiones de $1000 - $3000 por semana... Comi...   \n \n                                                 BODY MODELED_EXPIRED  \\\n 0  31-May-2024\\n\\nEnterprise Analyst (II-III)\\n\\n...        6/8/2024   \n 1  Oracle Consultant - Reports (3592)\\n\\nat SMX i...        8/1/2024   \n 2  Taking care of people is at the heart of every...       6/10/2024   \n 3  About this role:\\n\\nWells Fargo is looking for...       6/12/2024   \n 4  Comisiones de $1000 - $3000 por semana... Comi...       6/17/2024   \n \n    MODELED_DURATION  ...                             NAICS_2022_3_NAME  \\\n 0               6.0  ...               Motor Vehicle and Parts Dealers   \n 1              16.0  ...           Administrative and Support Services   \n 2               8.0  ...     Insurance Carriers and Related Activities   \n 3              10.0  ...  Credit Intermediation and Related Activities   \n 4              15.0  ...                         Unclassified Industry   \n \n   NAICS_2022_4                                  NAICS_2022_4_NAME  \\\n 0       4413.0  Automotive Parts, Accessories, and Tire Retailers   \n 1       5613.0                                Employment Services   \n 2       5242.0  Agencies, Brokerages, and Other Insurance Rela...   \n 3       5221.0                   Depository Credit Intermediation   \n 4       9999.0                              Unclassified Industry   \n \n   NAICS_2022_5                           NAICS_2022_5_NAME NAICS_2022_6  \\\n 0      44133.0  Automotive Parts and Accessories Retailers     441330.0   \n 1      56132.0                     Temporary Help Services     561320.0   \n 2      52429.0          Other Insurance Related Activities     524291.0   \n 3      52211.0                          Commercial Banking     522110.0   \n 4      99999.0                       Unclassified Industry     999999.0   \n \n                             NAICS_2022_6_NAME PAY_PERIOD  ANNUAL_SALARY  \\\n 0  Automotive Parts and Accessories Retailers       year            NaN   \n 1                     Temporary Help Services       year            NaN   \n 2                            Claims Adjusting       year            NaN   \n 3                          Commercial Banking       year            NaN   \n 4                       Unclassified Industry       year        92500.0   \n \n      month  \n 0  2024-06  \n 1  2024-06  \n 2  2024-06  \n 3  2024-06  \n 4  2024-06  \n \n [5 rows x 115 columns],\n (69198, 115))\n```\n:::\n:::\n\n\n</div>\n\n---\n\n<div class=\"card reveal\">\n\n## Summary\n\nThis cleaned dataset establishes the foundation for all downstream analysis.\n\nBy explicitly cleaning and preserving salary fields, selectively dropping only non-essential sparse columns, standardizing dates, and removing duplicates, we ensure:\n\n- accuracy and reproducibility,\n  \n- consistent salary-based insights,\n  \n- reduced rendering load for GitHub Pages,\n  \n- and smooth integration with the ML and NLP pipelines.\n\nThis completes the Data Cleaning & Pre-Processing phase.\n\n</div>\n\n",
    "supporting": [
      "data_cleaning_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}